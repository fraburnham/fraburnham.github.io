#+TITLE: Understanding CPU Bandwidth Throttling with the Completely Fair Scheduler and Control Groups
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="org-style.css" />
#+OPTIONS: toc:nil

NOTE: This is for the linux kernel version 6.1.

NOTE: I have no idea what I'm talking about.

TODO Overview of how each section joins to make throttling work (use a mermaid diagram! https://github.com/arnm/ob-mermaid)

#+TOC: headlines 1 local

** How Consumed CPU Bandwidth is Calculated and Tracked for ~cgroups~

~update_curr~ is called, at least, when a ~sched_entity~ is enqueued or dequeued (by ~enqueue_entity~ and ~dequeue_entity~ respectively). It calls ~cgroup_account_cputime~ which handles
propagating the consumed bandwitdth to all entities in the ~cgroup~.

From ~include/linux/sched.h~ TODO: cut boring stuff out of ~task_struct~

#+name: task_struct
#+begin_src c
  struct task_struct {
  #ifdef CONFIG_THREAD_INFO_IN_TASK
  	/*
  	 ,* For reasons of header soup (see current_thread_info()), this
  	 ,* must be the first element of task_struct.
  	 ,*/
  	struct thread_info		thread_info;
  #endif
  	unsigned int			__state;

  #ifdef CONFIG_PREEMPT_RT
  	/* saved state for "spinlock sleepers" */
  	unsigned int			saved_state;
  #endif

  	/*
  	 ,* This begins the randomizable portion of task_struct. Only
  	 ,* scheduling-critical items should be added above here.
  	 ,*/
  	randomized_struct_fields_start

  	void				*stack;
  	refcount_t			usage;
  	/* Per task flags (PF_*), defined further below: */
  	unsigned int			flags;
  	unsigned int			ptrace;

  #ifdef CONFIG_SMP
  	int				on_cpu;
  	struct __call_single_node	wake_entry;
  	unsigned int			wakee_flips;
  	unsigned long			wakee_flip_decay_ts;
  	struct task_struct		*last_wakee;

  	/*
  	 ,* recent_used_cpu is initially set as the last CPU used by a task
  	 ,* that wakes affine another task. Waker/wakee relationships can
  	 ,* push tasks around a CPU where each wakeup moves to the next one.
  	 ,* Tracking a recently used CPU allows a quick search for a recently
  	 ,* used CPU that may be idle.
  	 ,*/
  	int				recent_used_cpu;
  	int				wake_cpu;
  #endif
  	int				on_rq;

  	int				prio;
  	int				static_prio;
  	int				normal_prio;
  	unsigned int			rt_priority;

  	struct sched_entity		se;
  	struct sched_rt_entity		rt;
  	struct sched_dl_entity		dl;
  	const struct sched_class	*sched_class;

  #ifdef CONFIG_SCHED_CORE
  	struct rb_node			core_node;
  	unsigned long			core_cookie;
  	unsigned int			core_occupation;
  #endif

  #ifdef CONFIG_CGROUP_SCHED
  	struct task_group		*sched_task_group;
  #endif

  #ifdef CONFIG_UCLAMP_TASK
  	/*
  	 ,* Clamp values requested for a scheduling entity.
  	 ,* Must be updated with task_rq_lock() held.
  	 ,*/
  	struct uclamp_se		uclamp_req[UCLAMP_CNT];
  	/*
  	 ,* Effective clamp values used for a scheduling entity.
  	 ,* Must be updated with task_rq_lock() held.
  	 ,*/
  	struct uclamp_se		uclamp[UCLAMP_CNT];
  #endif

  	struct sched_statistics         stats;

  #ifdef CONFIG_PREEMPT_NOTIFIERS
  	/* List of struct preempt_notifier: */
  	struct hlist_head		preempt_notifiers;
  #endif

  #ifdef CONFIG_BLK_DEV_IO_TRACE
  	unsigned int			btrace_seq;
  #endif

  	unsigned int			policy;
  	int				nr_cpus_allowed;
  	const cpumask_t			*cpus_ptr;
  	cpumask_t			*user_cpus_ptr;
  	cpumask_t			cpus_mask;
  	void				*migration_pending;
  #ifdef CONFIG_SMP
  	unsigned short			migration_disabled;
  #endif
  	unsigned short			migration_flags;

  #ifdef CONFIG_PREEMPT_RCU
  	int				rcu_read_lock_nesting;
  	union rcu_special		rcu_read_unlock_special;
  	struct list_head		rcu_node_entry;
  	struct rcu_node			*rcu_blocked_node;
  #endif /* #ifdef CONFIG_PREEMPT_RCU */

  #ifdef CONFIG_TASKS_RCU
  	unsigned long			rcu_tasks_nvcsw;
  	u8				rcu_tasks_holdout;
  	u8				rcu_tasks_idx;
  	int				rcu_tasks_idle_cpu;
  	struct list_head		rcu_tasks_holdout_list;
  #endif /* #ifdef CONFIG_TASKS_RCU */

  #ifdef CONFIG_TASKS_TRACE_RCU
  	int				trc_reader_nesting;
  	int				trc_ipi_to_cpu;
  	union rcu_special		trc_reader_special;
  	struct list_head		trc_holdout_list;
  	struct list_head		trc_blkd_node;
  	int				trc_blkd_cpu;
  #endif /* #ifdef CONFIG_TASKS_TRACE_RCU */

  	struct sched_info		sched_info;

  	struct list_head		tasks;
  #ifdef CONFIG_SMP
  	struct plist_node		pushable_tasks;
  	struct rb_node			pushable_dl_tasks;
  #endif

  	struct mm_struct		*mm;
  	struct mm_struct		*active_mm;

  #ifdef SPLIT_RSS_COUNTING
  	struct task_rss_stat		rss_stat;
  #endif
  	int				exit_state;
  	int				exit_code;
  	int				exit_signal;
  	/* The signal sent when the parent dies: */
  	int				pdeath_signal;
  	/* JOBCTL_*, siglock protected: */
  	unsigned long			jobctl;

  	/* Used for emulating ABI behavior of previous Linux versions: */
  	unsigned int			personality;

  	/* Scheduler bits, serialized by scheduler locks: */
  	unsigned			sched_reset_on_fork:1;
  	unsigned			sched_contributes_to_load:1;
  	unsigned			sched_migrated:1;
  #ifdef CONFIG_PSI
  	unsigned			sched_psi_wake_requeue:1;
  #endif

  	/* Force alignment to the next boundary: */
  	unsigned			:0;

  	/* Unserialized, strictly 'current' */

  	/*
  	 ,* This field must not be in the scheduler word above due to wakelist
  	 ,* queueing no longer being serialized by p->on_cpu. However:
  	 ,*
  	 ,* p->XXX = X;			ttwu()
  	 ,* schedule()			  if (p->on_rq && ..) // false
  	 ,*   smp_mb__after_spinlock();	  if (smp_load_acquire(&p->on_cpu) && //true
  	 ,*   deactivate_task()		      ttwu_queue_wakelist())
  	 ,*     p->on_rq = 0;			p->sched_remote_wakeup = Y;
  	 ,*
  	 ,* guarantees all stores of 'current' are visible before
  	 ,* ->sched_remote_wakeup gets used, so it can be in this word.
  	 ,*/
  	unsigned			sched_remote_wakeup:1;

  	/* Bit to tell LSMs we're in execve(): */
  	unsigned			in_execve:1;
  	unsigned			in_iowait:1;
  #ifndef TIF_RESTORE_SIGMASK
  	unsigned			restore_sigmask:1;
  #endif
  #ifdef CONFIG_MEMCG
  	unsigned			in_user_fault:1;
  #endif
  #ifdef CONFIG_LRU_GEN
  	/* whether the LRU algorithm may apply to this access */
  	unsigned			in_lru_fault:1;
  #endif
  #ifdef CONFIG_COMPAT_BRK
  	unsigned			brk_randomized:1;
  #endif
  #ifdef CONFIG_CGROUPS
  	/* disallow userland-initiated cgroup migration */
  	unsigned			no_cgroup_migration:1;
  	/* task is frozen/stopped (used by the cgroup freezer) */
  	unsigned			frozen:1;
  #endif
  #ifdef CONFIG_BLK_CGROUP
  	unsigned			use_memdelay:1;
  #endif
  #ifdef CONFIG_PSI
  	/* Stalled due to lack of memory */
  	unsigned			in_memstall:1;
  #endif
  #ifdef CONFIG_PAGE_OWNER
  	/* Used by page_owner=on to detect recursion in page tracking. */
  	unsigned			in_page_owner:1;
  #endif
  #ifdef CONFIG_EVENTFD
  	/* Recursion prevention for eventfd_signal() */
  	unsigned			in_eventfd:1;
  #endif
  #ifdef CONFIG_IOMMU_SVA
  	unsigned			pasid_activated:1;
  #endif
  #ifdef	CONFIG_CPU_SUP_INTEL
  	unsigned			reported_split_lock:1;
  #endif
  #ifdef CONFIG_TASK_DELAY_ACCT
  	/* delay due to memory thrashing */
  	unsigned                        in_thrashing:1;
  #endif

  	unsigned long			atomic_flags; /* Flags requiring atomic access. */

  	struct restart_block		restart_block;

  	pid_t				pid;
  	pid_t				tgid;

  #ifdef CONFIG_STACKPROTECTOR
  	/* Canary value for the -fstack-protector GCC feature: */
  	unsigned long			stack_canary;
  #endif
  	/*
  	 ,* Pointers to the (original) parent process, youngest child, younger sibling,
  	 ,* older sibling, respectively.  (p->father can be replaced with
  	 ,* p->real_parent->pid)
  	 ,*/

  	/* Real parent process: */
  	struct task_struct __rcu	*real_parent;

  	/* Recipient of SIGCHLD, wait4() reports: */
  	struct task_struct __rcu	*parent;

  	/*
  	 ,* Children/sibling form the list of natural children:
  	 ,*/
  	struct list_head		children;
  	struct list_head		sibling;
  	struct task_struct		*group_leader;

  	/*
  	 ,* 'ptraced' is the list of tasks this task is using ptrace() on.
  	 ,*
  	 ,* This includes both natural children and PTRACE_ATTACH targets.
  	 ,* 'ptrace_entry' is this task's link on the p->parent->ptraced list.
  	 ,*/
  	struct list_head		ptraced;
  	struct list_head		ptrace_entry;

  	/* PID/PID hash table linkage. */
  	struct pid			*thread_pid;
  	struct hlist_node		pid_links[PIDTYPE_MAX];
  	struct list_head		thread_group;
  	struct list_head		thread_node;

  	struct completion		*vfork_done;

  	/* CLONE_CHILD_SETTID: */
  	int __user			*set_child_tid;

  	/* CLONE_CHILD_CLEARTID: */
  	int __user			*clear_child_tid;

  	/* PF_KTHREAD | PF_IO_WORKER */
  	void				*worker_private;

  	u64				utime;
  	u64				stime;
  #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
  	u64				utimescaled;
  	u64				stimescaled;
  #endif
  	u64				gtime;
  	struct prev_cputime		prev_cputime;
  #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
  	struct vtime			vtime;
  #endif

  #ifdef CONFIG_NO_HZ_FULL
  	atomic_t			tick_dep_mask;
  #endif
  	/* Context switch counts: */
  	unsigned long			nvcsw;
  	unsigned long			nivcsw;

  	/* Monotonic time in nsecs: */
  	u64				start_time;

  	/* Boot based time in nsecs: */
  	u64				start_boottime;

  	/* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */
  	unsigned long			min_flt;
  	unsigned long			maj_flt;

  	/* Empty if CONFIG_POSIX_CPUTIMERS=n */
  	struct posix_cputimers		posix_cputimers;

  #ifdef CONFIG_POSIX_CPU_TIMERS_TASK_WORK
  	struct posix_cputimers_work	posix_cputimers_work;
  #endif

  	/* Process credentials: */

  	/* Tracer's credentials at attach: */
  	const struct cred __rcu		*ptracer_cred;

  	/* Objective and real subjective task credentials (COW): */
  	const struct cred __rcu		*real_cred;

  	/* Effective (overridable) subjective task credentials (COW): */
  	const struct cred __rcu		*cred;

  #ifdef CONFIG_KEYS
  	/* Cached requested key. */
  	struct key			*cached_requested_key;
  #endif

  	/*
  	 ,* executable name, excluding path.
  	 ,*
  	 ,* - normally initialized setup_new_exec()
  	 ,* - access it with [gs]et_task_comm()
  	 ,* - lock it with task_lock()
  	 ,*/
  	char				comm[TASK_COMM_LEN];

  	struct nameidata		*nameidata;

  #ifdef CONFIG_SYSVIPC
  	struct sysv_sem			sysvsem;
  	struct sysv_shm			sysvshm;
  #endif
  #ifdef CONFIG_DETECT_HUNG_TASK
  	unsigned long			last_switch_count;
  	unsigned long			last_switch_time;
  #endif
  	/* Filesystem information: */
  	struct fs_struct		*fs;

  	/* Open file information: */
  	struct files_struct		*files;

  #ifdef CONFIG_IO_URING
  	struct io_uring_task		*io_uring;
  #endif

  	/* Namespaces: */
  	struct nsproxy			*nsproxy;

  	/* Signal handlers: */
  	struct signal_struct		*signal;
  	struct sighand_struct __rcu		*sighand;
  	sigset_t			blocked;
  	sigset_t			real_blocked;
  	/* Restored if set_restore_sigmask() was used: */
  	sigset_t			saved_sigmask;
  	struct sigpending		pending;
  	unsigned long			sas_ss_sp;
  	size_t				sas_ss_size;
  	unsigned int			sas_ss_flags;

  	struct callback_head		*task_works;

  #ifdef CONFIG_AUDIT
  #ifdef CONFIG_AUDITSYSCALL
  	struct audit_context		*audit_context;
  #endif
  	kuid_t				loginuid;
  	unsigned int			sessionid;
  #endif
  	struct seccomp			seccomp;
  	struct syscall_user_dispatch	syscall_dispatch;

  	/* Thread group tracking: */
  	u64				parent_exec_id;
  	u64				self_exec_id;

  	/* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */
  	spinlock_t			alloc_lock;

  	/* Protection of the PI data structures: */
  	raw_spinlock_t			pi_lock;

  	struct wake_q_node		wake_q;

  #ifdef CONFIG_RT_MUTEXES
  	/* PI waiters blocked on a rt_mutex held by this task: */
  	struct rb_root_cached		pi_waiters;
  	/* Updated under owner's pi_lock and rq lock */
  	struct task_struct		*pi_top_task;
  	/* Deadlock detection and priority inheritance handling: */
  	struct rt_mutex_waiter		*pi_blocked_on;
  #endif

  #ifdef CONFIG_DEBUG_MUTEXES
  	/* Mutex deadlock detection: */
  	struct mutex_waiter		*blocked_on;
  #endif

  #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
  	int				non_block_count;
  #endif

  #ifdef CONFIG_TRACE_IRQFLAGS
  	struct irqtrace_events		irqtrace;
  	unsigned int			hardirq_threaded;
  	u64				hardirq_chain_key;
  	int				softirqs_enabled;
  	int				softirq_context;
  	int				irq_config;
  #endif
  #ifdef CONFIG_PREEMPT_RT
  	int				softirq_disable_cnt;
  #endif

  #ifdef CONFIG_LOCKDEP
  # define MAX_LOCK_DEPTH			48UL
  	u64				curr_chain_key;
  	int				lockdep_depth;
  	unsigned int			lockdep_recursion;
  	struct held_lock		held_locks[MAX_LOCK_DEPTH];
  #endif

  #if defined(CONFIG_UBSAN) && !defined(CONFIG_UBSAN_TRAP)
  	unsigned int			in_ubsan;
  #endif

  	/* Journalling filesystem info: */
  	void				*journal_info;

  	/* Stacked block device info: */
  	struct bio_list			*bio_list;

  	/* Stack plugging: */
  	struct blk_plug			*plug;

  	/* VM state: */
  	struct reclaim_state		*reclaim_state;

  	struct backing_dev_info		*backing_dev_info;

  	struct io_context		*io_context;

  #ifdef CONFIG_COMPACTION
  	struct capture_control		*capture_control;
  #endif
  	/* Ptrace state: */
  	unsigned long			ptrace_message;
  	kernel_siginfo_t		*last_siginfo;

  	struct task_io_accounting	ioac;
  #ifdef CONFIG_PSI
  	/* Pressure stall state */
  	unsigned int			psi_flags;
  #endif
  #ifdef CONFIG_TASK_XACCT
  	/* Accumulated RSS usage: */
  	u64				acct_rss_mem1;
  	/* Accumulated virtual memory usage: */
  	u64				acct_vm_mem1;
  	/* stime + utime since last update: */
  	u64				acct_timexpd;
  #endif
  #ifdef CONFIG_CPUSETS
  	/* Protected by ->alloc_lock: */
  	nodemask_t			mems_allowed;
  	/* Sequence number to catch updates: */
  	seqcount_spinlock_t		mems_allowed_seq;
  	int				cpuset_mem_spread_rotor;
  	int				cpuset_slab_spread_rotor;
  #endif
  #ifdef CONFIG_CGROUPS
  	/* Control Group info protected by css_set_lock: */
  	struct css_set __rcu		*cgroups;
  	/* cg_list protected by css_set_lock and tsk->alloc_lock: */
  	struct list_head		cg_list;
  #endif
  #ifdef CONFIG_X86_CPU_RESCTRL
  	u32				closid;
  	u32				rmid;
  #endif
  #ifdef CONFIG_FUTEX
  	struct robust_list_head __user	*robust_list;
  #ifdef CONFIG_COMPAT
  	struct compat_robust_list_head __user *compat_robust_list;
  #endif
  	struct list_head		pi_state_list;
  	struct futex_pi_state		*pi_state_cache;
  	struct mutex			futex_exit_mutex;
  	unsigned int			futex_state;
  #endif
  #ifdef CONFIG_PERF_EVENTS
  	struct perf_event_context	*perf_event_ctxp[perf_nr_task_contexts];
  	struct mutex			perf_event_mutex;
  	struct list_head		perf_event_list;
  #endif
  #ifdef CONFIG_DEBUG_PREEMPT
  	unsigned long			preempt_disable_ip;
  #endif
  #ifdef CONFIG_NUMA
  	/* Protected by alloc_lock: */
  	struct mempolicy		*mempolicy;
  	short				il_prev;
  	short				pref_node_fork;
  #endif
  #ifdef CONFIG_NUMA_BALANCING
  	int				numa_scan_seq;
  	unsigned int			numa_scan_period;
  	unsigned int			numa_scan_period_max;
  	int				numa_preferred_nid;
  	unsigned long			numa_migrate_retry;
  	/* Migration stamp: */
  	u64				node_stamp;
  	u64				last_task_numa_placement;
  	u64				last_sum_exec_runtime;
  	struct callback_head		numa_work;

  	/*
  	 ,* This pointer is only modified for current in syscall and
  	 ,* pagefault context (and for tasks being destroyed), so it can be read
  	 ,* from any of the following contexts:
  	 ,*  - RCU read-side critical section
  	 ,*  - current->numa_group from everywhere
  	 ,*  - task's runqueue locked, task not running
  	 ,*/
  	struct numa_group __rcu		*numa_group;

  	/*
  	 ,* numa_faults is an array split into four regions:
  	 ,* faults_memory, faults_cpu, faults_memory_buffer, faults_cpu_buffer
  	 ,* in this precise order.
  	 ,*
  	 ,* faults_memory: Exponential decaying average of faults on a per-node
  	 ,* basis. Scheduling placement decisions are made based on these
  	 ,* counts. The values remain static for the duration of a PTE scan.
  	 ,* faults_cpu: Track the nodes the process was running on when a NUMA
  	 ,* hinting fault was incurred.
  	 ,* faults_memory_buffer and faults_cpu_buffer: Record faults per node
  	 ,* during the current scan window. When the scan completes, the counts
  	 ,* in faults_memory and faults_cpu decay and these values are copied.
  	 ,*/
  	unsigned long			*numa_faults;
  	unsigned long			total_numa_faults;

  	/*
  	 ,* numa_faults_locality tracks if faults recorded during the last
  	 ,* scan window were remote/local or failed to migrate. The task scan
  	 ,* period is adapted based on the locality of the faults with different
  	 ,* weights depending on whether they were shared or private faults
  	 ,*/
  	unsigned long			numa_faults_locality[3];

  	unsigned long			numa_pages_migrated;
  #endif /* CONFIG_NUMA_BALANCING */

  #ifdef CONFIG_RSEQ
  	struct rseq __user *rseq;
  	u32 rseq_sig;
  	/*
  	 ,* RmW on rseq_event_mask must be performed atomically
  	 ,* with respect to preemption.
  	 ,*/
  	unsigned long rseq_event_mask;
  #endif

  	struct tlbflush_unmap_batch	tlb_ubc;

  	union {
  		refcount_t		rcu_users;
  		struct rcu_head		rcu;
  	};

  	/* Cache last used pipe for splice(): */
  	struct pipe_inode_info		*splice_pipe;

  	struct page_frag		task_frag;

  #ifdef CONFIG_TASK_DELAY_ACCT
  	struct task_delay_info		*delays;
  #endif

  #ifdef CONFIG_FAULT_INJECTION
  	int				make_it_fail;
  	unsigned int			fail_nth;
  #endif
  	/*
  	 ,* When (nr_dirtied >= nr_dirtied_pause), it's time to call
  	 ,* balance_dirty_pages() for a dirty throttling pause:
  	 ,*/
  	int				nr_dirtied;
  	int				nr_dirtied_pause;
  	/* Start of a write-and-pause period: */
  	unsigned long			dirty_paused_when;

  #ifdef CONFIG_LATENCYTOP
  	int				latency_record_count;
  	struct latency_record		latency_record[LT_SAVECOUNT];
  #endif
  	/*
  	 ,* Time slack values; these are used to round up poll() and
  	 ,* select() etc timeout values. These are in nanoseconds.
  	 ,*/
  	u64				timer_slack_ns;
  	u64				default_timer_slack_ns;

  #if defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)
  	unsigned int			kasan_depth;
  #endif

  #ifdef CONFIG_KCSAN
  	struct kcsan_ctx		kcsan_ctx;
  #ifdef CONFIG_TRACE_IRQFLAGS
  	struct irqtrace_events		kcsan_save_irqtrace;
  #endif
  #ifdef CONFIG_KCSAN_WEAK_MEMORY
  	int				kcsan_stack_depth;
  #endif
  #endif

  #ifdef CONFIG_KMSAN
  	struct kmsan_ctx		kmsan_ctx;
  #endif

  #if IS_ENABLED(CONFIG_KUNIT)
  	struct kunit			*kunit_test;
  #endif

  #ifdef CONFIG_FUNCTION_GRAPH_TRACER
  	/* Index of current stored address in ret_stack: */
  	int				curr_ret_stack;
  	int				curr_ret_depth;

  	/* Stack of return addresses for return function tracing: */
  	struct ftrace_ret_stack		*ret_stack;

  	/* Timestamp for last schedule: */
  	unsigned long long		ftrace_timestamp;

  	/*
  	 ,* Number of functions that haven't been traced
  	 ,* because of depth overrun:
  	 ,*/
  	atomic_t			trace_overrun;

  	/* Pause tracing: */
  	atomic_t			tracing_graph_pause;
  #endif

  #ifdef CONFIG_TRACING
  	/* Bitmask and counter of trace recursion: */
  	unsigned long			trace_recursion;
  #endif /* CONFIG_TRACING */

  #ifdef CONFIG_KCOV
  	/* See kernel/kcov.c for more details. */

  	/* Coverage collection mode enabled for this task (0 if disabled): */
  	unsigned int			kcov_mode;

  	/* Size of the kcov_area: */
  	unsigned int			kcov_size;

  	/* Buffer for coverage collection: */
  	void				*kcov_area;

  	/* KCOV descriptor wired with this task or NULL: */
  	struct kcov			*kcov;

  	/* KCOV common handle for remote coverage collection: */
  	u64				kcov_handle;

  	/* KCOV sequence number: */
  	int				kcov_sequence;

  	/* Collect coverage from softirq context: */
  	unsigned int			kcov_softirq;
  #endif

  #ifdef CONFIG_MEMCG
  	struct mem_cgroup		*memcg_in_oom;
  	gfp_t				memcg_oom_gfp_mask;
  	int				memcg_oom_order;

  	/* Number of pages to reclaim on returning to userland: */
  	unsigned int			memcg_nr_pages_over_high;

  	/* Used by memcontrol for targeted memcg charge: */
  	struct mem_cgroup		*active_memcg;
  #endif

  #ifdef CONFIG_BLK_CGROUP
  	struct request_queue		*throttle_queue;
  #endif

  #ifdef CONFIG_UPROBES
  	struct uprobe_task		*utask;
  #endif
  #if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)
  	unsigned int			sequential_io;
  	unsigned int			sequential_io_avg;
  #endif
  	struct kmap_ctrl		kmap_ctrl;
  #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
  	unsigned long			task_state_change;
  # ifdef CONFIG_PREEMPT_RT
  	unsigned long			saved_state_change;
  # endif
  #endif
  	int				pagefault_disabled;
  #ifdef CONFIG_MMU
  	struct task_struct		*oom_reaper_list;
  	struct timer_list		oom_reaper_timer;
  #endif
  #ifdef CONFIG_VMAP_STACK
  	struct vm_struct		*stack_vm_area;
  #endif
  #ifdef CONFIG_THREAD_INFO_IN_TASK
  	/* A live task holds one reference: */
  	refcount_t			stack_refcount;
  #endif
  #ifdef CONFIG_LIVEPATCH
  	int patch_state;
  #endif
  #ifdef CONFIG_SECURITY
  	/* Used by LSM modules for access restriction: */
  	void				*security;
  #endif
  #ifdef CONFIG_BPF_SYSCALL
  	/* Used by BPF task local storage */
  	struct bpf_local_storage __rcu	*bpf_storage;
  	/* Used for BPF run context */
  	struct bpf_run_ctx		*bpf_ctx;
  #endif

  #ifdef CONFIG_GCC_PLUGIN_STACKLEAK
  	unsigned long			lowest_stack;
  	unsigned long			prev_lowest_stack;
  #endif

  #ifdef CONFIG_X86_MCE
  	void __user			*mce_vaddr;
  	__u64				mce_kflags;
  	u64				mce_addr;
  	__u64				mce_ripv : 1,
  					mce_whole_page : 1,
  					__mce_reserved : 62;
  	struct callback_head		mce_kill_me;
  	int				mce_count;
  #endif

  #ifdef CONFIG_KRETPROBES
  	struct llist_head               kretprobe_instances;
  #endif
  #ifdef CONFIG_RETHOOK
  	struct llist_head               rethooks;
  #endif

  #ifdef CONFIG_ARCH_HAS_PARANOID_L1D_FLUSH
  	/*
  	 ,* If L1D flush is supported on mm context switch
  	 ,* then we use this callback head to queue kill work
  	 ,* to kill tasks that are not running on SMT disabled
  	 ,* cores
  	 ,*/
  	struct callback_head		l1d_flush_kill;
  #endif

  #ifdef CONFIG_RV
  	/*
  	 ,* Per-task RV monitor. Nowadays fixed in RV_PER_TASK_MONITORS.
  	 ,* If we find justification for more monitors, we can think
  	 ,* about adding more or developing a dynamic method. So far,
  	 ,* none of these are justified.
  	 ,*/
  	union rv_task_monitor		rv[RV_PER_TASK_MONITORS];
  #endif

  	/*
  	 ,* New fields for task_struct should be added above here, so that
  	 ,* they are included in the randomized portion of task_struct.
  	 ,*/
  	randomized_struct_fields_end

  	/* CPU-specific state of this task: */
  	struct thread_struct		thread;

  	/*
  	 ,* WARNING: on x86, 'thread_struct' contains a variable-sized
  	 ,* structure.  It *MUST* be at the end of 'task_struct'.
  	 ,*
  	 ,* Do not put anything below here!
  	 ,*/
  };
#+end_src

#+name: task_group
#+begin_src c
  struct task_group {
    // stuff omitted

  #ifdef CONFIG_FAIR_GROUP_SCHED
  	/* schedulable entities of this group on each CPU */
  	struct sched_entity	**se;
  	/* runqueue "owned" by this group on each CPU */
  	struct cfs_rq		**cfs_rq;
  	unsigned long		shares;

  	/* A positive value indicates that this is a SCHED_IDLE group. */
  	int			idle;

  #ifdef	CONFIG_SMP
  	/*
  	 ,* load_avg can be heavily contended at clock tick time, so put
  	 ,* it in its own cacheline separated from the fields above which
  	 ,* will also be accessed at each tick.
  	 ,*/
  	atomic_long_t		load_avg ____cacheline_aligned;
  #endif
  #endif
    
    // stuff omitted

          struct task_group	*parent;
  	struct list_head	siblings;
  	struct list_head	children;

    // stuff omitted

          struct cfs_bandwidth	cfs_bandwidth;

    // stuff omitted
  };

  // stuff omitted

  /* CFS-related fields in a runqueue */
  struct cfs_rq {
  	struct load_weight	load;
  	unsigned int		nr_running;
  	unsigned int		h_nr_running;      /* SCHED_{NORMAL,BATCH,IDLE} */
  	unsigned int		idle_nr_running;   /* SCHED_IDLE */
  	unsigned int		idle_h_nr_running; /* SCHED_IDLE */

  	u64			exec_clock;
  	u64			min_vruntime;
  #ifdef CONFIG_SCHED_CORE
  	unsigned int		forceidle_seq;
  	u64			min_vruntime_fi;
  #endif

  #ifndef CONFIG_64BIT
  	u64			min_vruntime_copy;
  #endif

  	struct rb_root_cached	tasks_timeline;

  	/*
  	 ,* 'curr' points to currently running entity on this cfs_rq.
  	 ,* It is set to NULL otherwise (i.e when none are currently running).
  	 ,*/
  	struct sched_entity	*curr;
  	struct sched_entity	*next;
  	struct sched_entity	*last;
  	struct sched_entity	*skip;

    // stuff omitted

  #ifdef CONFIG_SMP
  	/*
  	 ,* CFS load tracking
  	 ,*/
  	struct sched_avg	avg;
  #ifndef CONFIG_64BIT
  	u64			last_update_time_copy;
  #endif
  	struct {
  		raw_spinlock_t	lock ____cacheline_aligned;
  		int		nr;
  		unsigned long	load_avg;
  		unsigned long	util_avg;
  		unsigned long	runnable_avg;
  	} removed;

  #ifdef CONFIG_FAIR_GROUP_SCHED
  	unsigned long		tg_load_avg_contrib;
  	long			propagate;
  	long			prop_runnable_sum;

  	/*
  	 ,*   h_load = weight * f(tg)
  	 ,*
  	 ,* Where f(tg) is the recursive weight fraction assigned to
  	 ,* this group.
  	 ,*/
  	unsigned long		h_load;
  	u64			last_h_load_update;
  	struct sched_entity	*h_load_next;
  #endif /* CONFIG_FAIR_GROUP_SCHED */
  #endif /* CONFIG_SMP */

  #ifdef CONFIG_FAIR_GROUP_SCHED
  	struct rq		*rq;	/* CPU runqueue to which this cfs_rq is attached */

  	/*
  	 ,* leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
  	 ,* a hierarchy). Non-leaf lrqs hold other higher schedulable entities
  	 ,* (like users, containers etc.)
  	 ,*
  	 ,* leaf_cfs_rq_list ties together list of leaf cfs_rq's in a CPU.
  	 ,* This list is used during load balance.
  	 ,*/
  	int			on_list;
  	struct list_head	leaf_cfs_rq_list;
  	struct task_group	*tg;	/* group that "owns" this runqueue */

  	/* Locally cached copy of our task_group's idle value */
  	int			idle;

  #ifdef CONFIG_CFS_BANDWIDTH
  	int			runtime_enabled;
  	s64			runtime_remaining;

  	u64			throttled_pelt_idle;
  #ifndef CONFIG_64BIT
  	u64                     throttled_pelt_idle_copy;
  #endif
  	u64			throttled_clock;
  	u64			throttled_clock_pelt;
  	u64			throttled_clock_pelt_time;
  	int			throttled;
  	int			throttle_count;
  	struct list_head	throttled_list;
  #endif /* CONFIG_CFS_BANDWIDTH */
  #endif /* CONFIG_FAIR_GROUP_SCHED */
  };
#+end_src

The ~task_group~ knows about the ~sched_entities~ and ~cfs_rqs~ that the group uses (one per cpu that the group uses). It also keeps track of the ~cfs_bandwidth~ which is used when
determining if a ~cgroup~ should be throttled. Each ~cfs_rq~ is also keeps track of bandwidth consumed.

From ~kernel/sched/fair.c~ (cfs scheduler)

#+name: update_curr
#+begin_src c
  /*
   * Update the current task's runtime statistics.
   */
  static void update_curr(struct cfs_rq *cfs_rq)
  {
  	struct sched_entity *curr = cfs_rq->curr;
  	u64 now = rq_clock_task(rq_of(cfs_rq));
  	u64 delta_exec;

  	if (unlikely(!curr))
  		return;

  	delta_exec = now - curr->exec_start;
  	if (unlikely((s64)delta_exec <= 0))
  		return;

  	curr->exec_start = now;

  	if (schedstat_enabled()) {
  		struct sched_statistics *stats;

  		stats = __schedstats_from_se(curr);
  		__schedstat_set(stats->exec_max,
  				max(delta_exec, stats->exec_max));
  	}

  	curr->sum_exec_runtime += delta_exec;
  	schedstat_add(cfs_rq->exec_clock, delta_exec);

  	curr->vruntime += calc_delta_fair(delta_exec, curr);
  	update_min_vruntime(cfs_rq);

  	if (entity_is_task(curr)) {
  		struct task_struct *curtask = task_of(curr);

  		trace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);
  		cgroup_account_cputime(curtask, delta_exec);
  		account_group_exec_runtime(curtask, delta_exec);
  	}

  	account_cfs_rq_runtime(cfs_rq, delta_exec);
  }
#+end_src

~update_curr~ is called for a variety of reasons like a task being enqueued or dequeued. It updates the ~sched_entity~ (the smallest scheduleable unit) that is currently running (or
task if the current entity is a task) in this ~cfs_rq~. It calls ~cgroup_account_cputime~ to handle tracking bandwidth usage for a ~cgroup~.

From ~include/linux/cgroup.h~

#+name: cgroup_acccount_cputime
#+begin_src c
  static inline void cgroup_account_cputime(struct task_struct *task,
  					  u64 delta_exec)
  {
  	struct cgroup *cgrp;

  	cpuacct_charge(task, delta_exec);

  	cgrp = task_dfl_cgroup(task);
  	if (cgroup_parent(cgrp))
  		__cgroup_account_cputime(cgrp, delta_exec);
  }
#+end_src

Given a task struct and a change in execution time, ~cgroup_account_cputime~ charges the task for the consumed bandwidth. If the task is a member of a group and it is not the parent,
handle group accounting.

From ~kernel/sched/cpuacct.c~

#+name: cpuacct_charge
#+begin_src c
  /* track CPU usage of a group of tasks and its child groups */
  struct cpuacct {
  	struct cgroup_subsys_state	css;
  	/* cpuusage holds pointer to a u64-type object on every CPU */
  	u64 __percpu	*cpuusage;
  	struct kernel_cpustat __percpu	*cpustat;
  };

  // stuff omitted

  void cpuacct_charge(struct task_struct *tsk, u64 cputime)
  {
  	unsigned int cpu = task_cpu(tsk);
  	struct cpuacct *ca;

  	lockdep_assert_rq_held(cpu_rq(cpu));

  	for (ca = task_ca(tsk); ca; ca = parent_ca(ca))
  		,*per_cpu_ptr(ca->cpuusage, cpu) += cputime;
  }
#+end_src

~cpuacct_charge~ charges a task by locking the run queue for the cpu the task was on (possibly to avoid possible changes to the real cputime value). Then for each task in this group
of tasks (not cgroup) charges the given ~cputime~ (which comes to ~cgroup_account_cputime~, the caller, as the change in execution time for the task).

From ~include/linux/cgroup-defs.h~ (resource statistics) and ~kernel/cgroup/rstat.c~

#+name: cgroup_rstat_cpu
#+begin_src c
  struct cgroup_base_stat {
  	struct task_cputime cputime;

  #ifdef CONFIG_SCHED_CORE
  	u64 forceidle_sum;
  #endif
  };

  /*
   * rstat - cgroup scalable recursive statistics.  Accounting is done
   * per-cpu in cgroup_rstat_cpu which is then lazily propagated up the
   * hierarchy on reads.
   *
   * When a stat gets updated, the cgroup_rstat_cpu and its ancestors are
   * linked into the updated tree.  On the following read, propagation only
   * considers and consumes the updated tree.  This makes reading O(the
   * number of descendants which have been active since last read) instead of
   * O(the total number of descendants).
   *
   * This is important because there can be a lot of (draining) cgroups which
   * aren't active and stat may be read frequently.  The combination can
   * become very expensive.  By propagating selectively, increasing reading
   * frequency decreases the cost of each read.
   *
   * This struct hosts both the fields which implement the above -
   * updated_children and updated_next - and the fields which track basic
   * resource statistics on top of it - bsync, bstat and last_bstat.
   */
  struct cgroup_rstat_cpu {
  	/*
  	 * ->bsync protects ->bstat.  These are the only fields which get
  	 * updated in the hot path.
  	 */
  	struct u64_stats_sync bsync;
  	struct cgroup_base_stat bstat;

  	/*
  	 * Snapshots at the last reading.  These are used to calculate the
  	 * deltas to propagate to the global counters.
  	 */
  	struct cgroup_base_stat last_bstat;

  	/*
  	 * Child cgroups with stat updates on this cpu since the last read
  	 * are linked on the parent's ->updated_children through
  	 * ->updated_next.
  	 *
  	 * In addition to being more compact, singly-linked list pointing
  	 * to the cgroup makes it unnecessary for each per-cpu struct to
  	 * point back to the associated cgroup.
  	 *
  	 * Protected by per-cpu cgroup_rstat_cpu_lock.
  	 */
  	struct cgroup *updated_children;	/* terminated by self cgroup */
  	struct cgroup *updated_next;		/* NULL iff not on the list */
  };
#+end_src

Looks like ~cgroup_rstat_cpu~ fits in by providing some structure to track a cgroups cpu resource statistics.

#+name: __cgroup_account_cputime
#+begin_src c
  void __cgroup_account_cputime(struct cgroup *cgrp, u64 delta_exec)
  {
  	struct cgroup_rstat_cpu *rstatc;
  	unsigned long flags;

  	rstatc = cgroup_base_stat_cputime_account_begin(cgrp, &flags);
  	rstatc->bstat.cputime.sum_exec_runtime += delta_exec;
  	cgroup_base_stat_cputime_account_end(cgrp, rstatc, flags);
  }
#+end_src

~__cgroup_accocunt_cputime~ updates the stats for a ~cgroup~ by getting the ~rstatc~ (in what looks like a locking manner) and adds the change in runtime to the current ~sum_exec_runtime~


** TODO: How ~cgroups~ are Throttled

TODO I see the ~cfs_rq~ throttle machinery. Is cgroups stuff connected to that? I think the ~cfs_rq~ throttle may be related do involuntary context switches...

There *must* be somewhere that cgroup limits are told to the cfs. How else could it throttle? Find the cgroup struct and look for info there.

From ~include/linux/sched.h~ TODO: cut boring stuff out of ~task_struct~

#+name: task_struct
#+begin_src c
  struct task_struct {
  #ifdef CONFIG_THREAD_INFO_IN_TASK
  	/*
  	 ,* For reasons of header soup (see current_thread_info()), this
  	 ,* must be the first element of task_struct.
  	 ,*/
  	struct thread_info		thread_info;
  #endif
  	unsigned int			__state;

  #ifdef CONFIG_PREEMPT_RT
  	/* saved state for "spinlock sleepers" */
  	unsigned int			saved_state;
  #endif

  	/*
  	 ,* This begins the randomizable portion of task_struct. Only
  	 ,* scheduling-critical items should be added above here.
  	 ,*/
  	randomized_struct_fields_start

  	void				*stack;
  	refcount_t			usage;
  	/* Per task flags (PF_*), defined further below: */
  	unsigned int			flags;
  	unsigned int			ptrace;

  #ifdef CONFIG_SMP
  	int				on_cpu;
  	struct __call_single_node	wake_entry;
  	unsigned int			wakee_flips;
  	unsigned long			wakee_flip_decay_ts;
  	struct task_struct		*last_wakee;

  	/*
  	 ,* recent_used_cpu is initially set as the last CPU used by a task
  	 ,* that wakes affine another task. Waker/wakee relationships can
  	 ,* push tasks around a CPU where each wakeup moves to the next one.
  	 ,* Tracking a recently used CPU allows a quick search for a recently
  	 ,* used CPU that may be idle.
  	 ,*/
  	int				recent_used_cpu;
  	int				wake_cpu;
  #endif
  	int				on_rq;

  	int				prio;
  	int				static_prio;
  	int				normal_prio;
  	unsigned int			rt_priority;

  	struct sched_entity		se;
  	struct sched_rt_entity		rt;
  	struct sched_dl_entity		dl;
  	const struct sched_class	*sched_class;

  #ifdef CONFIG_SCHED_CORE
  	struct rb_node			core_node;
  	unsigned long			core_cookie;
  	unsigned int			core_occupation;
  #endif

  #ifdef CONFIG_CGROUP_SCHED
  	struct task_group		*sched_task_group;
  #endif

  #ifdef CONFIG_UCLAMP_TASK
  	/*
  	 ,* Clamp values requested for a scheduling entity.
  	 ,* Must be updated with task_rq_lock() held.
  	 ,*/
  	struct uclamp_se		uclamp_req[UCLAMP_CNT];
  	/*
  	 ,* Effective clamp values used for a scheduling entity.
  	 ,* Must be updated with task_rq_lock() held.
  	 ,*/
  	struct uclamp_se		uclamp[UCLAMP_CNT];
  #endif

  	struct sched_statistics         stats;

  #ifdef CONFIG_PREEMPT_NOTIFIERS
  	/* List of struct preempt_notifier: */
  	struct hlist_head		preempt_notifiers;
  #endif

  #ifdef CONFIG_BLK_DEV_IO_TRACE
  	unsigned int			btrace_seq;
  #endif

  	unsigned int			policy;
  	int				nr_cpus_allowed;
  	const cpumask_t			*cpus_ptr;
  	cpumask_t			*user_cpus_ptr;
  	cpumask_t			cpus_mask;
  	void				*migration_pending;
  #ifdef CONFIG_SMP
  	unsigned short			migration_disabled;
  #endif
  	unsigned short			migration_flags;

  #ifdef CONFIG_PREEMPT_RCU
  	int				rcu_read_lock_nesting;
  	union rcu_special		rcu_read_unlock_special;
  	struct list_head		rcu_node_entry;
  	struct rcu_node			*rcu_blocked_node;
  #endif /* #ifdef CONFIG_PREEMPT_RCU */

  #ifdef CONFIG_TASKS_RCU
  	unsigned long			rcu_tasks_nvcsw;
  	u8				rcu_tasks_holdout;
  	u8				rcu_tasks_idx;
  	int				rcu_tasks_idle_cpu;
  	struct list_head		rcu_tasks_holdout_list;
  #endif /* #ifdef CONFIG_TASKS_RCU */

  #ifdef CONFIG_TASKS_TRACE_RCU
  	int				trc_reader_nesting;
  	int				trc_ipi_to_cpu;
  	union rcu_special		trc_reader_special;
  	struct list_head		trc_holdout_list;
  	struct list_head		trc_blkd_node;
  	int				trc_blkd_cpu;
  #endif /* #ifdef CONFIG_TASKS_TRACE_RCU */

  	struct sched_info		sched_info;

  	struct list_head		tasks;
  #ifdef CONFIG_SMP
  	struct plist_node		pushable_tasks;
  	struct rb_node			pushable_dl_tasks;
  #endif

  	struct mm_struct		*mm;
  	struct mm_struct		*active_mm;

  #ifdef SPLIT_RSS_COUNTING
  	struct task_rss_stat		rss_stat;
  #endif
  	int				exit_state;
  	int				exit_code;
  	int				exit_signal;
  	/* The signal sent when the parent dies: */
  	int				pdeath_signal;
  	/* JOBCTL_*, siglock protected: */
  	unsigned long			jobctl;

  	/* Used for emulating ABI behavior of previous Linux versions: */
  	unsigned int			personality;

  	/* Scheduler bits, serialized by scheduler locks: */
  	unsigned			sched_reset_on_fork:1;
  	unsigned			sched_contributes_to_load:1;
  	unsigned			sched_migrated:1;
  #ifdef CONFIG_PSI
  	unsigned			sched_psi_wake_requeue:1;
  #endif

  	/* Force alignment to the next boundary: */
  	unsigned			:0;

  	/* Unserialized, strictly 'current' */

  	/*
  	 ,* This field must not be in the scheduler word above due to wakelist
  	 ,* queueing no longer being serialized by p->on_cpu. However:
  	 ,*
  	 ,* p->XXX = X;			ttwu()
  	 ,* schedule()			  if (p->on_rq && ..) // false
  	 ,*   smp_mb__after_spinlock();	  if (smp_load_acquire(&p->on_cpu) && //true
  	 ,*   deactivate_task()		      ttwu_queue_wakelist())
  	 ,*     p->on_rq = 0;			p->sched_remote_wakeup = Y;
  	 ,*
  	 ,* guarantees all stores of 'current' are visible before
  	 ,* ->sched_remote_wakeup gets used, so it can be in this word.
  	 ,*/
  	unsigned			sched_remote_wakeup:1;

  	/* Bit to tell LSMs we're in execve(): */
  	unsigned			in_execve:1;
  	unsigned			in_iowait:1;
  #ifndef TIF_RESTORE_SIGMASK
  	unsigned			restore_sigmask:1;
  #endif
  #ifdef CONFIG_MEMCG
  	unsigned			in_user_fault:1;
  #endif
  #ifdef CONFIG_LRU_GEN
  	/* whether the LRU algorithm may apply to this access */
  	unsigned			in_lru_fault:1;
  #endif
  #ifdef CONFIG_COMPAT_BRK
  	unsigned			brk_randomized:1;
  #endif
  #ifdef CONFIG_CGROUPS
  	/* disallow userland-initiated cgroup migration */
  	unsigned			no_cgroup_migration:1;
  	/* task is frozen/stopped (used by the cgroup freezer) */
  	unsigned			frozen:1;
  #endif
  #ifdef CONFIG_BLK_CGROUP
  	unsigned			use_memdelay:1;
  #endif
  #ifdef CONFIG_PSI
  	/* Stalled due to lack of memory */
  	unsigned			in_memstall:1;
  #endif
  #ifdef CONFIG_PAGE_OWNER
  	/* Used by page_owner=on to detect recursion in page tracking. */
  	unsigned			in_page_owner:1;
  #endif
  #ifdef CONFIG_EVENTFD
  	/* Recursion prevention for eventfd_signal() */
  	unsigned			in_eventfd:1;
  #endif
  #ifdef CONFIG_IOMMU_SVA
  	unsigned			pasid_activated:1;
  #endif
  #ifdef	CONFIG_CPU_SUP_INTEL
  	unsigned			reported_split_lock:1;
  #endif
  #ifdef CONFIG_TASK_DELAY_ACCT
  	/* delay due to memory thrashing */
  	unsigned                        in_thrashing:1;
  #endif

  	unsigned long			atomic_flags; /* Flags requiring atomic access. */

  	struct restart_block		restart_block;

  	pid_t				pid;
  	pid_t				tgid;

  #ifdef CONFIG_STACKPROTECTOR
  	/* Canary value for the -fstack-protector GCC feature: */
  	unsigned long			stack_canary;
  #endif
  	/*
  	 ,* Pointers to the (original) parent process, youngest child, younger sibling,
  	 ,* older sibling, respectively.  (p->father can be replaced with
  	 ,* p->real_parent->pid)
  	 ,*/

  	/* Real parent process: */
  	struct task_struct __rcu	*real_parent;

  	/* Recipient of SIGCHLD, wait4() reports: */
  	struct task_struct __rcu	*parent;

  	/*
  	 ,* Children/sibling form the list of natural children:
  	 ,*/
  	struct list_head		children;
  	struct list_head		sibling;
  	struct task_struct		*group_leader;

  	/*
  	 ,* 'ptraced' is the list of tasks this task is using ptrace() on.
  	 ,*
  	 ,* This includes both natural children and PTRACE_ATTACH targets.
  	 ,* 'ptrace_entry' is this task's link on the p->parent->ptraced list.
  	 ,*/
  	struct list_head		ptraced;
  	struct list_head		ptrace_entry;

  	/* PID/PID hash table linkage. */
  	struct pid			*thread_pid;
  	struct hlist_node		pid_links[PIDTYPE_MAX];
  	struct list_head		thread_group;
  	struct list_head		thread_node;

  	struct completion		*vfork_done;

  	/* CLONE_CHILD_SETTID: */
  	int __user			*set_child_tid;

  	/* CLONE_CHILD_CLEARTID: */
  	int __user			*clear_child_tid;

  	/* PF_KTHREAD | PF_IO_WORKER */
  	void				*worker_private;

  	u64				utime;
  	u64				stime;
  #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
  	u64				utimescaled;
  	u64				stimescaled;
  #endif
  	u64				gtime;
  	struct prev_cputime		prev_cputime;
  #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
  	struct vtime			vtime;
  #endif

  #ifdef CONFIG_NO_HZ_FULL
  	atomic_t			tick_dep_mask;
  #endif
  	/* Context switch counts: */
  	unsigned long			nvcsw;
  	unsigned long			nivcsw;

  	/* Monotonic time in nsecs: */
  	u64				start_time;

  	/* Boot based time in nsecs: */
  	u64				start_boottime;

  	/* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */
  	unsigned long			min_flt;
  	unsigned long			maj_flt;

  	/* Empty if CONFIG_POSIX_CPUTIMERS=n */
  	struct posix_cputimers		posix_cputimers;

  #ifdef CONFIG_POSIX_CPU_TIMERS_TASK_WORK
  	struct posix_cputimers_work	posix_cputimers_work;
  #endif

  	/* Process credentials: */

  	/* Tracer's credentials at attach: */
  	const struct cred __rcu		*ptracer_cred;

  	/* Objective and real subjective task credentials (COW): */
  	const struct cred __rcu		*real_cred;

  	/* Effective (overridable) subjective task credentials (COW): */
  	const struct cred __rcu		*cred;

  #ifdef CONFIG_KEYS
  	/* Cached requested key. */
  	struct key			*cached_requested_key;
  #endif

  	/*
  	 ,* executable name, excluding path.
  	 ,*
  	 ,* - normally initialized setup_new_exec()
  	 ,* - access it with [gs]et_task_comm()
  	 ,* - lock it with task_lock()
  	 ,*/
  	char				comm[TASK_COMM_LEN];

  	struct nameidata		*nameidata;

  #ifdef CONFIG_SYSVIPC
  	struct sysv_sem			sysvsem;
  	struct sysv_shm			sysvshm;
  #endif
  #ifdef CONFIG_DETECT_HUNG_TASK
  	unsigned long			last_switch_count;
  	unsigned long			last_switch_time;
  #endif
  	/* Filesystem information: */
  	struct fs_struct		*fs;

  	/* Open file information: */
  	struct files_struct		*files;

  #ifdef CONFIG_IO_URING
  	struct io_uring_task		*io_uring;
  #endif

  	/* Namespaces: */
  	struct nsproxy			*nsproxy;

  	/* Signal handlers: */
  	struct signal_struct		*signal;
  	struct sighand_struct __rcu		*sighand;
  	sigset_t			blocked;
  	sigset_t			real_blocked;
  	/* Restored if set_restore_sigmask() was used: */
  	sigset_t			saved_sigmask;
  	struct sigpending		pending;
  	unsigned long			sas_ss_sp;
  	size_t				sas_ss_size;
  	unsigned int			sas_ss_flags;

  	struct callback_head		*task_works;

  #ifdef CONFIG_AUDIT
  #ifdef CONFIG_AUDITSYSCALL
  	struct audit_context		*audit_context;
  #endif
  	kuid_t				loginuid;
  	unsigned int			sessionid;
  #endif
  	struct seccomp			seccomp;
  	struct syscall_user_dispatch	syscall_dispatch;

  	/* Thread group tracking: */
  	u64				parent_exec_id;
  	u64				self_exec_id;

  	/* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */
  	spinlock_t			alloc_lock;

  	/* Protection of the PI data structures: */
  	raw_spinlock_t			pi_lock;

  	struct wake_q_node		wake_q;

  #ifdef CONFIG_RT_MUTEXES
  	/* PI waiters blocked on a rt_mutex held by this task: */
  	struct rb_root_cached		pi_waiters;
  	/* Updated under owner's pi_lock and rq lock */
  	struct task_struct		*pi_top_task;
  	/* Deadlock detection and priority inheritance handling: */
  	struct rt_mutex_waiter		*pi_blocked_on;
  #endif

  #ifdef CONFIG_DEBUG_MUTEXES
  	/* Mutex deadlock detection: */
  	struct mutex_waiter		*blocked_on;
  #endif

  #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
  	int				non_block_count;
  #endif

  #ifdef CONFIG_TRACE_IRQFLAGS
  	struct irqtrace_events		irqtrace;
  	unsigned int			hardirq_threaded;
  	u64				hardirq_chain_key;
  	int				softirqs_enabled;
  	int				softirq_context;
  	int				irq_config;
  #endif
  #ifdef CONFIG_PREEMPT_RT
  	int				softirq_disable_cnt;
  #endif

  #ifdef CONFIG_LOCKDEP
  # define MAX_LOCK_DEPTH			48UL
  	u64				curr_chain_key;
  	int				lockdep_depth;
  	unsigned int			lockdep_recursion;
  	struct held_lock		held_locks[MAX_LOCK_DEPTH];
  #endif

  #if defined(CONFIG_UBSAN) && !defined(CONFIG_UBSAN_TRAP)
  	unsigned int			in_ubsan;
  #endif

  	/* Journalling filesystem info: */
  	void				*journal_info;

  	/* Stacked block device info: */
  	struct bio_list			*bio_list;

  	/* Stack plugging: */
  	struct blk_plug			*plug;

  	/* VM state: */
  	struct reclaim_state		*reclaim_state;

  	struct backing_dev_info		*backing_dev_info;

  	struct io_context		*io_context;

  #ifdef CONFIG_COMPACTION
  	struct capture_control		*capture_control;
  #endif
  	/* Ptrace state: */
  	unsigned long			ptrace_message;
  	kernel_siginfo_t		*last_siginfo;

  	struct task_io_accounting	ioac;
  #ifdef CONFIG_PSI
  	/* Pressure stall state */
  	unsigned int			psi_flags;
  #endif
  #ifdef CONFIG_TASK_XACCT
  	/* Accumulated RSS usage: */
  	u64				acct_rss_mem1;
  	/* Accumulated virtual memory usage: */
  	u64				acct_vm_mem1;
  	/* stime + utime since last update: */
  	u64				acct_timexpd;
  #endif
  #ifdef CONFIG_CPUSETS
  	/* Protected by ->alloc_lock: */
  	nodemask_t			mems_allowed;
  	/* Sequence number to catch updates: */
  	seqcount_spinlock_t		mems_allowed_seq;
  	int				cpuset_mem_spread_rotor;
  	int				cpuset_slab_spread_rotor;
  #endif
  #ifdef CONFIG_CGROUPS
  	/* Control Group info protected by css_set_lock: */
  	struct css_set __rcu		*cgroups;
  	/* cg_list protected by css_set_lock and tsk->alloc_lock: */
  	struct list_head		cg_list;
  #endif
  #ifdef CONFIG_X86_CPU_RESCTRL
  	u32				closid;
  	u32				rmid;
  #endif
  #ifdef CONFIG_FUTEX
  	struct robust_list_head __user	*robust_list;
  #ifdef CONFIG_COMPAT
  	struct compat_robust_list_head __user *compat_robust_list;
  #endif
  	struct list_head		pi_state_list;
  	struct futex_pi_state		*pi_state_cache;
  	struct mutex			futex_exit_mutex;
  	unsigned int			futex_state;
  #endif
  #ifdef CONFIG_PERF_EVENTS
  	struct perf_event_context	*perf_event_ctxp[perf_nr_task_contexts];
  	struct mutex			perf_event_mutex;
  	struct list_head		perf_event_list;
  #endif
  #ifdef CONFIG_DEBUG_PREEMPT
  	unsigned long			preempt_disable_ip;
  #endif
  #ifdef CONFIG_NUMA
  	/* Protected by alloc_lock: */
  	struct mempolicy		*mempolicy;
  	short				il_prev;
  	short				pref_node_fork;
  #endif
  #ifdef CONFIG_NUMA_BALANCING
  	int				numa_scan_seq;
  	unsigned int			numa_scan_period;
  	unsigned int			numa_scan_period_max;
  	int				numa_preferred_nid;
  	unsigned long			numa_migrate_retry;
  	/* Migration stamp: */
  	u64				node_stamp;
  	u64				last_task_numa_placement;
  	u64				last_sum_exec_runtime;
  	struct callback_head		numa_work;

  	/*
  	 ,* This pointer is only modified for current in syscall and
  	 ,* pagefault context (and for tasks being destroyed), so it can be read
  	 ,* from any of the following contexts:
  	 ,*  - RCU read-side critical section
  	 ,*  - current->numa_group from everywhere
  	 ,*  - task's runqueue locked, task not running
  	 ,*/
  	struct numa_group __rcu		*numa_group;

  	/*
  	 ,* numa_faults is an array split into four regions:
  	 ,* faults_memory, faults_cpu, faults_memory_buffer, faults_cpu_buffer
  	 ,* in this precise order.
  	 ,*
  	 ,* faults_memory: Exponential decaying average of faults on a per-node
  	 ,* basis. Scheduling placement decisions are made based on these
  	 ,* counts. The values remain static for the duration of a PTE scan.
  	 ,* faults_cpu: Track the nodes the process was running on when a NUMA
  	 ,* hinting fault was incurred.
  	 ,* faults_memory_buffer and faults_cpu_buffer: Record faults per node
  	 ,* during the current scan window. When the scan completes, the counts
  	 ,* in faults_memory and faults_cpu decay and these values are copied.
  	 ,*/
  	unsigned long			*numa_faults;
  	unsigned long			total_numa_faults;

  	/*
  	 ,* numa_faults_locality tracks if faults recorded during the last
  	 ,* scan window were remote/local or failed to migrate. The task scan
  	 ,* period is adapted based on the locality of the faults with different
  	 ,* weights depending on whether they were shared or private faults
  	 ,*/
  	unsigned long			numa_faults_locality[3];

  	unsigned long			numa_pages_migrated;
  #endif /* CONFIG_NUMA_BALANCING */

  #ifdef CONFIG_RSEQ
  	struct rseq __user *rseq;
  	u32 rseq_sig;
  	/*
  	 ,* RmW on rseq_event_mask must be performed atomically
  	 ,* with respect to preemption.
  	 ,*/
  	unsigned long rseq_event_mask;
  #endif

  	struct tlbflush_unmap_batch	tlb_ubc;

  	union {
  		refcount_t		rcu_users;
  		struct rcu_head		rcu;
  	};

  	/* Cache last used pipe for splice(): */
  	struct pipe_inode_info		*splice_pipe;

  	struct page_frag		task_frag;

  #ifdef CONFIG_TASK_DELAY_ACCT
  	struct task_delay_info		*delays;
  #endif

  #ifdef CONFIG_FAULT_INJECTION
  	int				make_it_fail;
  	unsigned int			fail_nth;
  #endif
  	/*
  	 ,* When (nr_dirtied >= nr_dirtied_pause), it's time to call
  	 ,* balance_dirty_pages() for a dirty throttling pause:
  	 ,*/
  	int				nr_dirtied;
  	int				nr_dirtied_pause;
  	/* Start of a write-and-pause period: */
  	unsigned long			dirty_paused_when;

  #ifdef CONFIG_LATENCYTOP
  	int				latency_record_count;
  	struct latency_record		latency_record[LT_SAVECOUNT];
  #endif
  	/*
  	 ,* Time slack values; these are used to round up poll() and
  	 ,* select() etc timeout values. These are in nanoseconds.
  	 ,*/
  	u64				timer_slack_ns;
  	u64				default_timer_slack_ns;

  #if defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)
  	unsigned int			kasan_depth;
  #endif

  #ifdef CONFIG_KCSAN
  	struct kcsan_ctx		kcsan_ctx;
  #ifdef CONFIG_TRACE_IRQFLAGS
  	struct irqtrace_events		kcsan_save_irqtrace;
  #endif
  #ifdef CONFIG_KCSAN_WEAK_MEMORY
  	int				kcsan_stack_depth;
  #endif
  #endif

  #ifdef CONFIG_KMSAN
  	struct kmsan_ctx		kmsan_ctx;
  #endif

  #if IS_ENABLED(CONFIG_KUNIT)
  	struct kunit			*kunit_test;
  #endif

  #ifdef CONFIG_FUNCTION_GRAPH_TRACER
  	/* Index of current stored address in ret_stack: */
  	int				curr_ret_stack;
  	int				curr_ret_depth;

  	/* Stack of return addresses for return function tracing: */
  	struct ftrace_ret_stack		*ret_stack;

  	/* Timestamp for last schedule: */
  	unsigned long long		ftrace_timestamp;

  	/*
  	 ,* Number of functions that haven't been traced
  	 ,* because of depth overrun:
  	 ,*/
  	atomic_t			trace_overrun;

  	/* Pause tracing: */
  	atomic_t			tracing_graph_pause;
  #endif

  #ifdef CONFIG_TRACING
  	/* Bitmask and counter of trace recursion: */
  	unsigned long			trace_recursion;
  #endif /* CONFIG_TRACING */

  #ifdef CONFIG_KCOV
  	/* See kernel/kcov.c for more details. */

  	/* Coverage collection mode enabled for this task (0 if disabled): */
  	unsigned int			kcov_mode;

  	/* Size of the kcov_area: */
  	unsigned int			kcov_size;

  	/* Buffer for coverage collection: */
  	void				*kcov_area;

  	/* KCOV descriptor wired with this task or NULL: */
  	struct kcov			*kcov;

  	/* KCOV common handle for remote coverage collection: */
  	u64				kcov_handle;

  	/* KCOV sequence number: */
  	int				kcov_sequence;

  	/* Collect coverage from softirq context: */
  	unsigned int			kcov_softirq;
  #endif

  #ifdef CONFIG_MEMCG
  	struct mem_cgroup		*memcg_in_oom;
  	gfp_t				memcg_oom_gfp_mask;
  	int				memcg_oom_order;

  	/* Number of pages to reclaim on returning to userland: */
  	unsigned int			memcg_nr_pages_over_high;

  	/* Used by memcontrol for targeted memcg charge: */
  	struct mem_cgroup		*active_memcg;
  #endif

  #ifdef CONFIG_BLK_CGROUP
  	struct request_queue		*throttle_queue;
  #endif

  #ifdef CONFIG_UPROBES
  	struct uprobe_task		*utask;
  #endif
  #if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)
  	unsigned int			sequential_io;
  	unsigned int			sequential_io_avg;
  #endif
  	struct kmap_ctrl		kmap_ctrl;
  #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
  	unsigned long			task_state_change;
  # ifdef CONFIG_PREEMPT_RT
  	unsigned long			saved_state_change;
  # endif
  #endif
  	int				pagefault_disabled;
  #ifdef CONFIG_MMU
  	struct task_struct		*oom_reaper_list;
  	struct timer_list		oom_reaper_timer;
  #endif
  #ifdef CONFIG_VMAP_STACK
  	struct vm_struct		*stack_vm_area;
  #endif
  #ifdef CONFIG_THREAD_INFO_IN_TASK
  	/* A live task holds one reference: */
  	refcount_t			stack_refcount;
  #endif
  #ifdef CONFIG_LIVEPATCH
  	int patch_state;
  #endif
  #ifdef CONFIG_SECURITY
  	/* Used by LSM modules for access restriction: */
  	void				*security;
  #endif
  #ifdef CONFIG_BPF_SYSCALL
  	/* Used by BPF task local storage */
  	struct bpf_local_storage __rcu	*bpf_storage;
  	/* Used for BPF run context */
  	struct bpf_run_ctx		*bpf_ctx;
  #endif

  #ifdef CONFIG_GCC_PLUGIN_STACKLEAK
  	unsigned long			lowest_stack;
  	unsigned long			prev_lowest_stack;
  #endif

  #ifdef CONFIG_X86_MCE
  	void __user			*mce_vaddr;
  	__u64				mce_kflags;
  	u64				mce_addr;
  	__u64				mce_ripv : 1,
  					mce_whole_page : 1,
  					__mce_reserved : 62;
  	struct callback_head		mce_kill_me;
  	int				mce_count;
  #endif

  #ifdef CONFIG_KRETPROBES
  	struct llist_head               kretprobe_instances;
  #endif
  #ifdef CONFIG_RETHOOK
  	struct llist_head               rethooks;
  #endif

  #ifdef CONFIG_ARCH_HAS_PARANOID_L1D_FLUSH
  	/*
  	 ,* If L1D flush is supported on mm context switch
  	 ,* then we use this callback head to queue kill work
  	 ,* to kill tasks that are not running on SMT disabled
  	 ,* cores
  	 ,*/
  	struct callback_head		l1d_flush_kill;
  #endif

  #ifdef CONFIG_RV
  	/*
  	 ,* Per-task RV monitor. Nowadays fixed in RV_PER_TASK_MONITORS.
  	 ,* If we find justification for more monitors, we can think
  	 ,* about adding more or developing a dynamic method. So far,
  	 ,* none of these are justified.
  	 ,*/
  	union rv_task_monitor		rv[RV_PER_TASK_MONITORS];
  #endif

  	/*
  	 ,* New fields for task_struct should be added above here, so that
  	 ,* they are included in the randomized portion of task_struct.
  	 ,*/
  	randomized_struct_fields_end

  	/* CPU-specific state of this task: */
  	struct thread_struct		thread;

  	/*
  	 ,* WARNING: on x86, 'thread_struct' contains a variable-sized
  	 ,* structure.  It *MUST* be at the end of 'task_struct'.
  	 ,*
  	 ,* Do not put anything below here!
  	 ,*/
  };
#+end_src

~task_struct~ as a member ~cgroups~ that is a ~css_set~. It appears to be where the task "knows" about the ~cgroup~ it belongs to.

From ~include/linux/sched.h~

#+begin_src c
  #ifdef CONFIG_CGROUP_SCHED
  	struct task_group		*sched_task_group;
  #endif
#+end_src

TODO: The ~task_group~ has a ~sched_task_group~ that may be relevant

From ~kernel/sched/fair.c~

#+begin_src c
  #ifdef CONFIG_FAIR_GROUP_SCHED

  static bool __update_blocked_fair(struct rq *rq, bool *done)
  {
  	struct cfs_rq *cfs_rq, *pos;
  	bool decayed = false;
  	int cpu = cpu_of(rq);

  	/*
  	 ,* Iterates the task_group tree in a bottom up fashion, see
  	 ,* list_add_leaf_cfs_rq() for details.
  	 ,*/
  	for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {
  		struct sched_entity *se;

  		if (update_cfs_rq_load_avg(cfs_rq_clock_pelt(cfs_rq), cfs_rq)) {
  			update_tg_load_avg(cfs_rq);

  			if (cfs_rq->nr_running == 0)
  				update_idle_cfs_rq_clock_pelt(cfs_rq);

  			if (cfs_rq == &rq->cfs)
  				decayed = true;
  		}

  		/* Propagate pending load changes to the parent, if any: */
  		se = cfs_rq->tg->se[cpu];
  		if (se && !skip_blocked_update(se))
  			update_load_avg(cfs_rq_of(se), se, UPDATE_TG);

  		/*
  		 ,* There can be a lot of idle CPU cgroups.  Don't let fully
  		 ,* decayed cfs_rqs linger on the list.
  		 ,*/
  		if (cfs_rq_is_decayed(cfs_rq))
  			list_del_leaf_cfs_rq(cfs_rq);

  		/* Don't need periodic decay once load/util_avg are null */
  		if (cfs_rq_has_blocked(cfs_rq))
  			,*done = false;
  	}

  	return decayed;
  }

  /*
   ,* Compute the hierarchical load factor for cfs_rq and all its ascendants.
   ,* This needs to be done in a top-down fashion because the load of a child
   ,* group is a fraction of its parents load.
   ,*/
  static void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)
  {
  	struct rq *rq = rq_of(cfs_rq);
  	struct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];
  	unsigned long now = jiffies;
  	unsigned long load;

  	if (cfs_rq->last_h_load_update == now)
  		return;

  	WRITE_ONCE(cfs_rq->h_load_next, NULL);
  	for_each_sched_entity(se) {
  		cfs_rq = cfs_rq_of(se);
  		WRITE_ONCE(cfs_rq->h_load_next, se);
  		if (cfs_rq->last_h_load_update == now)
  			break;
  	}

  	if (!se) {
  		cfs_rq->h_load = cfs_rq_load_avg(cfs_rq);
  		cfs_rq->last_h_load_update = now;
  	}

  	while ((se = READ_ONCE(cfs_rq->h_load_next)) != NULL) {
  		load = cfs_rq->h_load;
  		load = div64_ul(load * se->avg.load_avg,
  			cfs_rq_load_avg(cfs_rq) + 1);
  		cfs_rq = group_cfs_rq(se);
  		cfs_rq->h_load = load;
  		cfs_rq->last_h_load_update = now;
  	}
  }

  static unsigned long task_h_load(struct task_struct *p)
  {
  	struct cfs_rq *cfs_rq = task_cfs_rq(p);

  	update_cfs_rq_h_load(cfs_rq);
  	return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
  			cfs_rq_load_avg(cfs_rq) + 1);
  }
  #else
  // stuff omitted
  #endif
#+end_src

TODO: Are the ~CONFIG_FAIR_GROUP_SCHED~ things related to cgroups? HMM! Perhaps yes, but maybe ~cgroups~ create ~task_groups~ at some /other/ level.

From ~cgroup-defs.h~
~struct cgroup_base_stat bstat;~ I remember seeing ~bstat~ before... ~/* cgroup basic resource statistics */~

~	/* used to store eBPF programs */~	~struct cgroup_bpf bpf;~ WHAT! Cgroup specific eBPF?!

What is cgroup freezing? Probably not relevant.

How do cgroups get their limits?

~cgroup_migrate~ looks like it adds a process/task to an existing ~cgroup~, or maybe to a controller? Is a controller "per cgroup" or "per system"?

~cgroup_attach_task~ to attach a task/threadgroup to a ~cgroup~

~init_css_set~ ?

~cgroup_post_fork~ may be a good place to start to see how the world of cgroups is set up

~__update_blocked_fair~ ?

~propagate_entity_cfs_rq~

~CONFIG_CGROUP_SCHED~ ~CONFIG_FAIR_GROUP_SCHED~ ~CONFIG_CGROUPS~

#+begin_src c
  #ifdef CONFIG_CGROUP_SCHED
  	struct task_group		*sched_task_group;
  #endif
#+end_src

#+begin_src c
  #ifdef CONFIG_CGROUPS
  	/* disallow userland-initiated cgroup migration */
  	unsigned			no_cgroup_migration:1;
  	/* task is frozen/stopped (used by the cgroup freezer) */
  	unsigned			frozen:1;
  #endif
#+end_src

#+begin_src c
  #ifdef CONFIG_CGROUPS
  	/* Control Group info protected by css_set_lock: */
  	struct css_set __rcu		*cgroups;
  	/* cg_list protected by css_set_lock and tsk->alloc_lock: */
  	struct list_head		cg_list;
  #endif
#+end_src

from sched/core.c
#+begin_src c
  	/*
  	 ,* We must load prev->state once (task_struct::state is volatile), such
  	 ,* that we form a control dependency vs deactivate_task() below.
  	 ,*/
  	prev_state = READ_ONCE(prev->__state);
  	if (!(sched_mode & SM_MASK_PREEMPT) && prev_state) {
  		if (signal_pending_state(prev_state, prev)) {
  			WRITE_ONCE(prev->__state, TASK_RUNNING);
  		} else {
  			prev->sched_contributes_to_load =
  				(prev_state & TASK_UNINTERRUPTIBLE) &&
  				!(prev_state & TASK_NOLOAD) &&
  				!(prev_state & TASK_FROZEN);
#+end_src

Is this were a task is skipped? Who sets that it is frozen?

#+begin_src c
  /*
   ,* Task state bitmask. NOTE! These bits are also
   ,* encoded in fs/proc/array.c: get_task_state().
   ,*
   ,* We have two separate sets of flags: task->state
   ,* is about runnability, while task->exit_state are
   ,* about the task exiting. Confusing, but this way
   ,* modifying one set can't modify the other one by
   ,* mistake.
   ,*/

  /* Used in tsk->state: */
  #define TASK_RUNNING			0x00000000
  #define TASK_INTERRUPTIBLE		0x00000001
  #define TASK_UNINTERRUPTIBLE		0x00000002
  #define __TASK_STOPPED			0x00000004
  #define __TASK_TRACED			0x00000008
  /* Used in tsk->exit_state: */
  #define EXIT_DEAD			0x00000010
  #define EXIT_ZOMBIE			0x00000020
  #define EXIT_TRACE			(EXIT_ZOMBIE | EXIT_DEAD)
  /* Used in tsk->state again: */
  #define TASK_PARKED			0x00000040
  #define TASK_DEAD			0x00000080
  #define TASK_WAKEKILL			0x00000100
  #define TASK_WAKING			0x00000200
  #define TASK_NOLOAD			0x00000400
  #define TASK_NEW			0x00000800
  #define TASK_RTLOCK_WAIT		0x00001000
  #define TASK_FREEZABLE			0x00002000
  #define __TASK_FREEZABLE_UNSAFE	       (0x00004000 * IS_ENABLED(CONFIG_LOCKDEP))
  #define TASK_FROZEN			0x00008000
  #define TASK_STATE_MAX			0x00010000

  #define TASK_ANY			(TASK_STATE_MAX-1)
#+end_src

I think the freezer is how the cgroups interact with the cfs... maybe... I still don't see where the max values are set.

~kernel/sched/psi.c~ ~Pressure stall information for CPU, memory and IO~ READ THE COMMENTS!. Not perfectly helpful. May still be useful.

Where do max values make it into the cgroup? Where do max values make it from the cgroup to the bandwidth calculation in the scheduler?

https://docs.kernel.org/scheduler/sched-bwc.html Thank god. Finally some docs that I needed. I should have started here. Ugh.

#+begin_src c
  struct cfs_bandwidth {
  #ifdef CONFIG_CFS_BANDWIDTH
  	raw_spinlock_t		lock;
  	ktime_t			period;
  	u64			quota;
  	u64			runtime;
  	u64			burst;
  	u64			runtime_snap;
  	s64			hierarchical_quota;

  	u8			idle;
  	u8			period_active;
  	u8			slack_started;
  	struct hrtimer		period_timer;
  	struct hrtimer		slack_timer;
  	struct list_head	throttled_cfs_rq;

  	/* Statistics: */
  	int			nr_periods;
  	int			nr_throttled;
  	int			nr_burst;
  	u64			throttled_time;
  	u64			burst_time;
  #endif
  };
#+end_src

Ok so this has to be where things are tracked, right? It's all over in fair.c. But where does it get populated? (Probably in fair.c or core.c. Probably in core.c)

~static int tg_set_cfs_bandwidth~

#+begin_src c
  static int tg_set_cfs_quota(struct task_group *tg, long cfs_quota_us)
  {
  	u64 quota, period, burst;

  	period = ktime_to_ns(tg->cfs_bandwidth.period);
  	burst = tg->cfs_bandwidth.burst;
  	if (cfs_quota_us < 0)
  		quota = RUNTIME_INF;
  	else if ((u64)cfs_quota_us <= U64_MAX / NSEC_PER_USEC)
  		quota = (u64)cfs_quota_us * NSEC_PER_USEC;
  	else
  		return -EINVAL;

  	return tg_set_cfs_bandwidth(tg, period, quota, burst);
  }

  // stuff omitted

  static int tg_set_cfs_period(struct task_group *tg, long cfs_period_us)
  {
  	u64 quota, period, burst;

  	if ((u64)cfs_period_us > U64_MAX / NSEC_PER_USEC)
  		return -EINVAL;

  	period = (u64)cfs_period_us * NSEC_PER_USEC;
  	quota = tg->cfs_bandwidth.quota;
  	burst = tg->cfs_bandwidth.burst;

  	return tg_set_cfs_bandwidth(tg, period, quota, burst);
  }
#+end_src

#+begin_src c
  static ssize_t cpu_max_write(struct kernfs_open_file *of,
  			     char *buf, size_t nbytes, loff_t off)
  {
  	struct task_group *tg = css_tg(of_css(of));
  	u64 period = tg_get_cfs_period(tg);
  	u64 burst = tg_get_cfs_burst(tg);
  	u64 quota;
  	int ret;

  	ret = cpu_period_quota_parse(buf, &period, &quota);
  	if (!ret)
  		ret = tg_set_cfs_bandwidth(tg, period, quota, burst);
  	return ret ?: nbytes;
  }
  #endif
#+end_src

Ok. So this is how a ~task_group~ knows its quota. How does it know which quota to look at?

Tasks can move between or leave ~css_set~s Is that where?

#+begin_src c
  static struct css_set *find_css_set(struct css_set *old_cset,
  				    struct cgroup *cgrp)
#+end_src

#+begin_src c
  /*
   ,* Return the cgroup for "task" from the given hierarchy. Must be
   ,* called with cgroup_mutex and css_set_lock held.
   ,*/
  struct cgroup *task_cgroup_from_root(struct task_struct *task,
  				     struct cgroup_root *root)
  {
  	/*
  	 ,* No need to lock the task - since we hold css_set_lock the
  	 ,* task can't change groups.
  	 ,*/
  	return cset_cgroup_from_root(task_css_set(task), root);
  }
#+end_src

~EXPORT_SYMBOL_GPL(task_cgroup_path);~ looks like some exported interface

#+begin_src c
  /**
   ,* cgroup_attach_task - attach a task or a whole threadgroup to a cgroup
   ,* @dst_cgrp: the cgroup to attach to
   ,* @leader: the task or the leader of the threadgroup to be attached
   ,* @threadgroup: attach the whole threadgroup?
   ,*
   ,* Call holding cgroup_mutex and cgroup_threadgroup_rwsem.
   ,*/
  int cgroup_attach_task(struct cgroup *dst_cgrp, struct task_struct *leader,
  		       bool threadgroup)
  {
  	DEFINE_CGROUP_MGCTX(mgctx);
  	struct task_struct *task;
  	int ret = 0;

  	/* look up all src csets */
  	spin_lock_irq(&css_set_lock);
  	rcu_read_lock();
  	task = leader;
  	do {
  		cgroup_migrate_add_src(task_css_set(task), dst_cgrp, &mgctx);
  		if (!threadgroup)
  			break;
  	} while_each_thread(leader, task);
  	rcu_read_unlock();
  	spin_unlock_irq(&css_set_lock);

  	/* prepare dst csets and commit */
  	ret = cgroup_migrate_prepare_dst(&mgctx);
  	if (!ret)
  		ret = cgroup_migrate(leader, threadgroup, &mgctx);

  	cgroup_migrate_finish(&mgctx);

  	if (!ret)
  		TRACE_CGROUP_PATH(attach_task, dst_cgrp, leader, threadgroup);

  	return ret;
  }
#+end_src

#+begin_src c
  /**
   ,* cgroup_enable_threaded - make @cgrp threaded
   ,* @cgrp: the target cgroup
   ,*
   ,* Called when "threaded" is written to the cgroup.type interface file and
   ,* tries to make @cgrp threaded and join the parent's resource domain.
   ,* This function is never called on the root cgroup as cgroup.type doesn't
   ,* exist on it.
   ,*/
  static int cgroup_enable_threaded(struct cgroup *cgrp)
  {
  	struct cgroup *parent = cgroup_parent(cgrp);
  	struct cgroup *dom_cgrp = parent->dom_cgrp;
  	struct cgroup *dsct;
  	struct cgroup_subsys_state *d_css;
  	int ret;

  	lockdep_assert_held(&cgroup_mutex);

  	/* noop if already threaded */
  	if (cgroup_is_threaded(cgrp))
  		return 0;

  	/*
  	 ,* If @cgroup is populated or has domain controllers enabled, it
  	 ,* can't be switched.  While the below cgroup_can_be_thread_root()
  	 ,* test can catch the same conditions, that's only when @parent is
  	 ,* not mixable, so let's check it explicitly.
  	 ,*/
  	if (cgroup_is_populated(cgrp) ||
  	    cgrp->subtree_control & ~cgrp_dfl_threaded_ss_mask)
  		return -EOPNOTSUPP;

  	/* we're joining the parent's domain, ensure its validity */
  	if (!cgroup_is_valid_domain(dom_cgrp) ||
  	    !cgroup_can_be_thread_root(dom_cgrp))
  		return -EOPNOTSUPP;

  	/*
  	 ,* The following shouldn't cause actual migrations and should
  	 ,* always succeed.
  	 ,*/
  	cgroup_save_control(cgrp);

  	cgroup_for_each_live_descendant_pre(dsct, d_css, cgrp)
  		if (dsct == cgrp || cgroup_is_threaded(dsct))
  			dsct->dom_cgrp = dom_cgrp;

  	ret = cgroup_apply_control(cgrp);
  	if (!ret)
  		parent->nr_threaded_children++;

  	cgroup_finalize_control(cgrp, ret);
  	return ret;
  }
#+end_src

#+begin_src c
  /*
   ,* proc_cgroup_show()
   ,*  - Print task's cgroup paths into seq_file, one line for each hierarchy
   ,*  - Used for /proc/<pid>/cgroup.
   ,*/
  int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
  		     struct pid *pid, struct task_struct *tsk)
  {
  	char *buf;
  	int retval;
  	struct cgroup_root *root;

  	retval = -ENOMEM;
  	buf = kmalloc(PATH_MAX, GFP_KERNEL);
  	if (!buf)
  		goto out;

  	mutex_lock(&cgroup_mutex);
  	spin_lock_irq(&css_set_lock);

  	for_each_root(root) {
  		struct cgroup_subsys *ss;
  		struct cgroup *cgrp;
  		int ssid, count = 0;

  		if (root == &cgrp_dfl_root && !READ_ONCE(cgrp_dfl_visible))
  			continue;

  		seq_printf(m, "%d:", root->hierarchy_id);
  		if (root != &cgrp_dfl_root)
  			for_each_subsys(ss, ssid)
  				if (root->subsys_mask & (1 << ssid))
  					seq_printf(m, "%s%s", count++ ? "," : "",
  						   ss->legacy_name);
  		if (strlen(root->name))
  			seq_printf(m, "%sname=%s", count ? "," : "",
  				   root->name);
  		seq_putc(m, ':');

  		cgrp = task_cgroup_from_root(tsk, root);

  		/*
  		 ,* On traditional hierarchies, all zombie tasks show up as
  		 ,* belonging to the root cgroup.  On the default hierarchy,
  		 ,* while a zombie doesn't show up in "cgroup.procs" and
  		 ,* thus can't be migrated, its /proc/PID/cgroup keeps
  		 ,* reporting the cgroup it belonged to before exiting.  If
  		 ,* the cgroup is removed before the zombie is reaped,
  		 ,* " (deleted)" is appended to the cgroup path.
  		 ,*/
  		if (cgroup_on_dfl(cgrp) || !(tsk->flags & PF_EXITING)) {
  			retval = cgroup_path_ns_locked(cgrp, buf, PATH_MAX,
  						current->nsproxy->cgroup_ns);
  			if (retval >= PATH_MAX)
  				retval = -ENAMETOOLONG;
  			if (retval < 0)
  				goto out_unlock;

  			seq_puts(m, buf);
  		} else {
  			seq_puts(m, "/");
  		}

  		if (cgroup_on_dfl(cgrp) && cgroup_is_dead(cgrp))
  			seq_puts(m, " (deleted)\n");
  		else
  			seq_putc(m, '\n');
  	}

  	retval = 0;
  out_unlock:
  	spin_unlock_irq(&css_set_lock);
  	mutex_unlock(&cgroup_mutex);
  	kfree(buf);
  out:
  	return retval;
  }
#+end_src
